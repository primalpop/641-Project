\documentclass[a4paper,11pt]{article}

\usepackage[big]{layaureo} 				%better formatting of the A4 page

\usepackage{graphics}
\usepackage{graphicx}

%Setup hyperref package, and colours for links
\usepackage{hyperref}

\begin{document}

%--------------------TITLE-------------

\title{Comparison of Topic modelling algorithms and evaluation of Separability assumption in Non-negative matrix Factorization}
\author{Primal Pappachan, Sunil Gandhi, Ravendar Lal \\ 
\texttt{\{primal1, sunilga1, rlal1\}@umbc.edu}}
\date{\today}
\maketitle

%--------------------SECTIONS----------------------------------


\section{Progress Report}
Following is a report of our progress in the project as of now.
\subsection{Progress so far}

\begin{enumerate}

\item As per suggested in proposal, we talked to Dr. Tim Oates on various aspects of this paper and he found our idea of approaching the problem of Topic Modeling quite convincing. He also suggested datasets for this task. Following are datasets that we can use for our problem of topic modelling:

\begin{itemize}
\item New York Times Dataset
\item AP Press Dataset
\item NIPS Dataset
\end{itemize}

\item The authors of paper have mentioned that they have not practically implemented the algorithm and except that versions of algorithms are practical. We are trying to explore this possibility, and are looking at how we can take probabilistic approach and concepts introduced by the paper like NMF and anchor words to find topic models.

\item After understanding the problem of Topic modeling in depth, we explored other methods of topic modelling like Latent Dirichlet allocation(LDA), Pachinko allocation model(PAM), Gibbs sampling and Correlated Topic Models(CTM) tried to understand the need of new methodology for topic modelling. We enumerated the assumptions they made and complexity of each model. 

\item We are using the scikits learn feature extraction module for extracting features from the document and represent the documents as bag of words. 

\item We are exploring several libraries for LDA model and trying it on datasets mentioned above. This will be helpful for comparison with results of NMF algorithm.

\end{enumerate}

\subsection{Remaining Work}
\begin{enumerate}
\item Coding Non-negative Matrix Factorization (NMF): We still have to implement NMF algorithm.
\item We are trying to explore appropriate metric for comparison of topic models and how we can implement these metric.
\item We have to check the validity of claim that anchor words assumption and that these words form convex hull around all the words in vocabulary.
\item Add TF-IDF weighting to NMF and compare its performance against LDA.
\item Based on the success of above listed works, we would like to further try to find similarity between documents using probability distributions of document over topics.
\end{enumerate}


\subsection{Problems being faced}
\begin{enumerate}
\item This algorithm is based on assumption that document by word matrix is generated by product of word by topic and topic by document matrix. It turns out that this document by word is a very noisy version of product. 
\item There are huge number of words and words "the" doesn't affect the topics generated. But they increase size of matrix and thus the running time of algorithm considerably. The proposed solution for this problem is to remove these words and use TF-IDF to run topic models for only words with certain TF-IDF score.
\item We are still trying to figure out the best way to evaluate performance of different topic models. 
\end{enumerate}


\subsection{Schedule}
Following is our schedule to tackle remaining tasks. 
\begin{table}
\centering
\begin{tabular}[htb]{l|r}
Task & Date\\\hline
Parsing Dataset & March 30 \\
Implementing LDA & April 2nd \\
Implementing NMF & April 8th \\
Result Comparison and Experimentation & April 15th \\
Final Report and Presentation & April 17th
\end{tabular}
\caption{\label{tab:widgets}Schedule.}
\end{table}

\pagebreak

\section{Introduction}
A huge number of documents are generated daily in form of journals, papers from conferences and  news paper articles. A large amount of unstructured data is already present on web in form of wikipedia, web pages and social network sites. A user usually cannot read all the data available to him. Also, reading through data to find out more about document is not a very feasible option considering the number of documents available. In situations like these, a system to understand the document and suggest users for relevant documents on certain topics or suggest the documents related to one that he is actually using would be of lot of benefit. We think that topic models can be helpful in performing such tasks. \\

Searching is another domain where topica models can be used. For example, topic models are ideal for searching on social media sites like quora. On quora there are large number of question and answer and it would be good if user could search through topics and find question and answers which are related to his topic of interest. Also, Finding advertisements which are related to query that user just searched and is probably interested in is an interesting problem whose roots lie in topic models. So, topic models have their application in various fields like advertisement, searching, recommendation engine etc. \\

[1] suggests that problem of topic modelling can be formulated as problem of factorization of matrix into non-negative matrices. The authors also suggest use of anchor words to make this problem tractable. But, no empirical results of NMF or LDA, which is another method of solving problem of topic models, is given. They have theoretically suggested that NMF is better than LDA and that it makes less severe assumption of separability. We will experimentally verify this claim and check for severity of "separability" assumption on real life datasets. We would also compare two documents based on their distribution over topic models. For this comparison, we can try different distance measures and understand which distance measure works best with topic models. This problem is significant because, understanding distance measures can be helpful in all types of machine learning problems like clustering, classification,etc. It will also be useful in understanding changes in document stream over time. Understanding these changes can be important for machine learning algorithms which work on this data as stated in [2]

\section{Previous Work}

The current approaches for learning Topic models use a Bayesian model based on the probability distribution of words inside a document{\cite{blei}}. Specifically the models used for document representation are

\begin{itemize} 
\item Pure model - based on the assumption that a document belongs to a single topic
\item Latent Dirichlet Allocation(LDA)
\item Corelated Topics model based on Pachinko Allocation
\end{itemize}

These models use algorithms which requires information about the maximum likelihood of observed data, which can be hard to compute. Also methods like computing the Singular Value Decomposition(SVD) of the document corpus for learning topic models can result in unnecessary computations and they might result in negative values of factored matrix for word and topic distributions as well. Instead, in this paper authors propose usage of NMF for learning topic models. They also suggest that if we assume existence of anchor words we can compute parameters of distribution of document in polynomial time. 

\section{Methods}


We will be implementing the Non-negative matrix factorization method for extracting topics from documents and building a topic and word distribution over the documents. The topics generated from the NMF module will be compared against categories available in the data set as well as the one generated by LDA model. We are evaluating various implementations of LDA for extracting topic models and will choose the one with highest percentage of accuracy in terms of accuracy. 

The separability assumption i.e anchor words existence will be verified by implementing the algorithm for identifying the anchor words for each topic. We will use either of KL divergence or quadratic loss as the objective function in recovering anchor words. In the former, the recovery procedure would be a maximum likelihood estimation problem in which we are trying to estimate the maximum probability of co-occurence of words. 

Finally the claim of matrix factorization performing better than LDA with TF-IDA weighting will be verified. Tf means term-frequency while tfâ€“idf means term-frequency times inverse document-frequency. By this technique the influence of common but unimportant words like "the", "a" etc will be adjusted so that they shadow influence of interesting terms which can be possible topics. 

\begin{thebibliography}{99}
\bibitem{tm} \textit{Learning Topic Models---Going beyond SVD}. Arora, Saneev, Rong Ge, and Ankur Moitra. In FOCS 2012.
\bibitem{dred} \textit{Online methods for multi-domain learning and adaptation}. Dredze, Mark and Crammer, Koby. In
EMNLP, 2008.
\bibitem{nmf} \textit{Computing a nonnegative matrix factorization--provably}. Sanjeev Arora, Rong Ge, Ravi Kannan, Ankur Moitra. Proceedings of the 44th symposium on Theory of Computing. ACM, 2012.  
\bibitem{blei} \textit{Review article on Probabilistic topic models}. David m. Blei  
\end{thebibliography}



\pagebreak


\end{document}