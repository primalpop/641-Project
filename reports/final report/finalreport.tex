\documentclass[a4paper,11pt]{article}

\usepackage[big]{layaureo} 				%better formatting of the A4 page

\usepackage{graphics}
\usepackage{graphicx}

%Setup hyperref package, and colours for links
\usepackage{hyperref}

\begin{document}

%--------------------TITLE-------------

\title{Comparison of Topic modelling algorithms and evaluation of Separability assumption in Non-negative matrix Factorization}
\author{Primal Pappachan, Sunil Gandhi, Ravendar Lal \\ 
\texttt{\{primal1, sunilga1, rlal1\}@umbc.edu}}
\date{\today}
\maketitle


\section{Introduction}
A huge number of documents are generated daily in form of journals, papers from conferences and  news paper articles. A large amount of unstructured data is already present on web in form of wikipedia, web pages and social network sites. A user usually cannot read all the data available to him. Also, reading through data to find out more about document is not a very feasible option considering the number of documents available. In situations like these, a system to understand the document and suggest users for relevant documents on certain topics or suggest the documents related to one that he is actually using would be of lot of benefit. We think that topic models can be helpful in performing such tasks. \\

Searching is another domain where topica models can be used. For example, topic models are ideal for searching on social media sites like quora. On quora there are large number of question and answer and it would be good if user could search through topics and find question and answers which are related to his topic of interest. Also, Finding advertisements which are related to query that user just searched and is probably interested in is an interesting problem whose roots lie in topic models. So, topic models have their application in various fields like advertisement, searching, recommendation engine etc. \\

[1] suggests that problem of topic modelling can be formulated as problem of factorization of matrix into non-negative matrices. The authors also suggest use of anchor words to make this problem tractable. But, no empirical results of NMF or LDA, which is another method of solving problem of topic models, is given. They have theoretically suggested that NMF is better than LDA and that it makes less severe assumption of separability. We will experimentally verify this claim and check for severity of "separability" assumption on real life datasets. We would also compare two documents based on their distribution over topic models. For this comparison, we can try different distance measures and understand which distance measure works best with topic models. This problem is significant because, understanding distance measures can be helpful in all types of machine learning problems like clustering, classification,etc. It will also be useful in understanding changes in document stream over time. Understanding these changes can be important for machine learning algorithms which work on this data as stated in [2]

\section{Previous Work}

The current approaches for learning Topic models use a Bayesian model based on the probability distribution of words inside a document{\cite{blei}}. Specifically the models used for document representation are

\begin{itemize} 
\item Pure model - based on the assumption that a document belongs to a single topic
\item Latent Dirichlet Allocation(LDA)
\item Corelated Topics model based on Pachinko Allocation
\end{itemize}

These models use algorithms which requires information about the maximum likelihood of observed data, which can be hard to compute. Also methods like computing the Singular Value Decomposition(SVD) of the document corpus for learning topic models can result in unnecessary computations and they might result in negative values of factored matrix for word and topic distributions as well. Instead, in this paper authors propose usage of NMF for learning topic models. They also suggest that if we assume existence of anchor words we can compute parameters of distribution of document in polynomial time. 

\section{Methods}


We will be implementing the Non-negative matrix factorization method for extracting topics from documents and building a topic and word distribution over the documents. The topics generated from the NMF module will be compared against categories available in the data set as well as the one generated by LDA model. We are evaluating various implementations of LDA for extracting topic models and will choose the one with highest percentage of accuracy in terms of accuracy. 

The separability assumption i.e anchor words existence will be verified by implementing the algorithm for identifying the anchor words for each topic. We will use either of KL divergence or quadratic loss as the objective function in recovering anchor words. In the former, the recovery procedure would be a maximum likelihood estimation problem in which we are trying to estimate the maximum probability of co-occurence of words. 

Finally the claim of matrix factorization performing better than LDA with TF-IDA weighting will be verified. Tf means term-frequency while tfâ€“idf means term-frequency times inverse document-frequency. By this technique the influence of common but unimportant words like "the", "a" etc will be adjusted so that they shadow influence of interesting terms which can be possible topics. 

\begin{thebibliography}{99}
\bibitem{tm} \textit{Learning Topic Models---Going beyond SVD}. Arora, Saneev, Rong Ge, and Ankur Moitra. In FOCS 2012.
\bibitem{dred} \textit{Online methods for multi-domain learning and adaptation}. Dredze, Mark and Crammer, Koby. In
EMNLP, 2008.
\bibitem{nmf} \textit{Computing a nonnegative matrix factorization--provably}. Sanjeev Arora, Rong Ge, Ravi Kannan, Ankur Moitra. Proceedings of the 44th symposium on Theory of Computing. ACM, 2012.  
\bibitem{blei} \textit{Review article on Probabilistic topic models}. David m. Blei  
\end{thebibliography}



\pagebreak


\end{document}