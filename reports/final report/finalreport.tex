\documentclass[a4paper,11pt]{article}

\usepackage[big]{layaureo} 				%better formatting of the A4 page

\usepackage{graphics}
\usepackage{graphicx}

%Setup hyperref package, and colours for links
\usepackage{hyperref}

\begin{document}

%--------------------TITLE-------------

\title{Comparison of Topic modelling algorithms and evaluation of Separability assumption in Non-negative matrix Factorization}
\author{Primal Pappachan, Sunil Gandhi, Ravendar Lal \\ 
\texttt{\{primal1, sunilga1, rlal1\}@umbc.edu}}
\date{\today}
\maketitle


\section{Introduction}
A huge number of documents are generated daily in form of journals, papers from conferences and newspaper articles. A large amount of unstructured data is already present on web in form of wikipedia, web pages and social network sites. A user usually cannot read all the data available to him. Also, reading through data to find out more about document is not a very feasible option considering the number of documents available. In these cases it is important to understand the document and suggest users for relevant documents on certain topics or it would benefit if suggest the documents related to one that he is actually using. We think that topic models can be helpful in performing such tasks. \\

Topic modeling is an unsupervised approach that learns thematic structure from documents. It is a general observation that given a collection of documents, there are atleast few words that helps to differentiate one document from other even if higher level topics are same like articles of sports category. Articles of football will differ from baseball based on keywords specific to its game. \\

The other applications where we can use topic models is during searching. For example, topic models are ideal for searching on social media sites like quora. On quora there are large number of question and answer and it would be good if user could search through topics and find question and answers which are related to his topic of interest. Organizations like New York Times present another example where topic modeling can be very handy. NY Times has articles since 1851. It’ll be next to impossible to tag them as tagging was not that common before emergence of computer. Methods like topic modeling can help to mine and classify these large corpus of documents. Also, finding advertisements which are related to query that user just searched and is probably interested in is an interesting problem whose roots lie in topic models. So, topic models have their application in various fields like advertisement, searching, recommendation engine, etc. \\

[1] Suggests that problem of topic modelling can be formulated as problem of factorization of matrix into non-negative matrices. It also suggests use of anchor words to make this problem tractable. But, no empirical results of NMF and LDA which is another method of solving problem of topic models is given. Although they theoretically, suggest that NMF is better than LDA and it makes less severe assumption of separability. We are experimentally verifying this claim and check for severity of "separability" assumption on real life dataset. We are also comparing two documents based on their distribution over topic models. For this comparison, we can try different distance measures and understand which distance measure works best with topic models. This problem is significant because, understanding distance measures can be helpful in all types of machine learning problems like clustering, classification etc. It will also be useful in understanding changes in document stream over time. Understanding these changes can be important for machine learning algorithms which work on this data as stated in [2]. \\

In this paper, we have experimented and verified the approaches that are theoretically mentioned in the paper. In following section, we have mentioned some of the previous and algorithms that are used to evaluate topic models. There are many different approaches but we have mentioned some of most prominent ones. After that you will see detailed discussion about our methods that we are using along with some of the preliminary results that we might be improving upon by final report. \\


\section{Previous Work}

The current approaches for learning Topic models use a Bayesian model based on the probability distribution of words inside a document{\cite{blei}}. Specifically the models used for document representation are

\begin{itemize} 
\item Pure model - based on the assumption that a document belongs to a single topic
\item Latent Dirichlet Allocation(LDA)
\item Corelated Topics model based on Pachinko Allocation
\end{itemize}

These models use algorithms which requires information about the maximum likelihood of observed data, which can be hard to compute. Also methods like computing the Singular Value Decomposition(SVD) of the document corpus for learning topic models can result in unnecessary computations and they might result in negative values of factored matrix for word and topic distributions as well. Instead, in this paper authors propose usage of NMF for learning topic models. They also suggest that if we assume existence of anchor words we can compute parameters of distribution of document in polynomial time. 

\section{Methods}


We will be implementing the Non-negative matrix factorization method for extracting topics from documents and building a topic and word distribution over the documents. The topics generated from the NMF module will be compared against categories available in the data set as well as the one generated by LDA model. We are evaluating various implementations of LDA for extracting topic models and will choose the one with highest percentage of accuracy in terms of accuracy. 

The separability assumption i.e anchor words existence will be verified by implementing the algorithm for identifying the anchor words for each topic. We will use either of KL divergence or quadratic loss as the objective function in recovering anchor words. In the former, the recovery procedure would be a maximum likelihood estimation problem in which we are trying to estimate the maximum probability of co-occurence of words. 

Finally the claim of matrix factorization performing better than LDA with TF-IDA weighting will be verified. Tf means term-frequency while tf–idf means term-frequency times inverse document-frequency. By this technique the influence of common but unimportant words like "the", "a" etc will be adjusted so that they shadow influence of interesting terms which can be possible topics. 

\begin{thebibliography}{99}
\bibitem{tm} \textit{Learning Topic Models---Going beyond SVD}. Arora, Saneev, Rong Ge, and Ankur Moitra. In FOCS 2012.
\bibitem{dred} \textit{Online methods for multi-domain learning and adaptation}. Dredze, Mark and Crammer, Koby. In
EMNLP, 2008.
\bibitem{nmf} \textit{Computing a nonnegative matrix factorization--provably}. Sanjeev Arora, Rong Ge, Ravi Kannan, Ankur Moitra. Proceedings of the 44th symposium on Theory of Computing. ACM, 2012.  
\bibitem{blei} \textit{Review article on Probabilistic topic models}. David m. Blei  
\end{thebibliography}



\pagebreak


\end{document}