\documentclass[a4paper,11pt]{article}

\usepackage[big]{layaureo} 				%better formatting of the A4 page

\usepackage{graphics}

%Setup hyperref package, and colours for links
\usepackage{hyperref}

\begin{document}

%--------------------TITLE-------------

\title{Topic Extraction and analysing Document Similarity}
\author{Primal Pappachan, Sunil Gandhi, Ravendar Lal \\ 
\texttt{primal1@umbc.edu, Add your ids}}
\date{\today}
\maketitle

%--------------------SECTIONS----------------------------------

\section{Project Summary}
\subsection{Description}
Measuring the similarity between two documents based on topic models built using Non-negative Matrix Factorization 
\subsection{}Keywords
Topic models, SVD, LDA, NMF, Document Similarity 
\subsection{Focus}
How does Non-negative Factorization(NMF) fare against Latent Dirichlet Allocation(LDA) for discovering topic models from a document? Can these topic models be used to analyse the similarity between a pair documents based on distance measures like KL and L-inf?
\subsection{Responsibilities}
\item Primal Pappachan
\item Sunil
\item Ravendar
\subsection{Total Budget} 

\subsection{Deliverables}

%List of deliverables
\begin{itemize}
\item Output
\end{itemize}


\section{Executive Summary}


\section{Motivation}


\section{Previous Work}

The current approaches for learning Topic models use a Bayesian model based on the probability distribution of words inside a document. Specifically the models used for document representation are

\begin{itemize} 
\item Pure model - based on the assumption that a document belongs to a single topic
\item Latent Dirichlet Allocation(LDA)
\item Corelated Topics model based on Pachinko Allocation
\end{itemize}

These models use algorithms which requires information about the maximum likelihood of observed data, can be hard to compute. Also the spectral methods, like computing the Singular Value Decomposition(SVD) of the document corpus for learning topic models can result in unnecessary computations(##positive and negatives). Instead, in this paper authors propose usage of NMF for learning topic models. This method with the anchor word assumption can compute discover the hidden structure in document i.e learn topics in polynomial time. 


\section{Specific Aims}

\begin{enumerate}
\item Understanding Topic models and Non-negative Matrix Factorization method
\item Implementation of NMF for topic extraction from documents
\item Comparing NMF results with Latent Dirichlet Allocation (LDA) approach
\item Experimenting with various Distance measures to find similarity between the documents
\item Using the distance measure chosen from previous step to compute similarity between documents
\end{enumerate}


\section{Plan}

First of all, we will implement NMF method for topic extraction from documents without anchor words assumption. Afterwards, this will be evaluated against NMF with anchor words for performance. 

The results from the previous step will be compare against traditional approaches like LDA used for learning topic models for performance and accuracy and possibly validate author's claim of NMF being better than LDA for topic models.

The NMF implementation will be put into use for measuring similarity between two documents. We will compare various distance measures for measuring similarity between probability distributions and after this step the relatively better measure would be chosen for our purpose.   

\section{Deliverables}

E.g., project report, PowerPoint slides, presentation, source code.

\section{Issues}
What difficulties do you foresee, and how do you plan to overcome them?

\section{Bibliography}
List (in proper bibliographic form) all works you need to complete your project.


\begin{itemize}
\item
\item
\end{itemize}  

\section{Schedule}

Timeline

\section{Budget}

What resources (including your own time) do you need to complete the project? Be sure to include
required wages, equipment, travel. Using a spreadsheet (e.g., Excel), summarize in one simple page your
direct, indirect, and total costs. As your indirect costs, include a 47\% overhead for UMBC on all direct
costs. (Indirect cost is UMBC’s overhead. Direct costs are everything else, including UMBC’s 33\% of
any salary for benefits.)


\section{Appendix A: Research Conference}
Attach a copy of the paper from a recent algorithms research conference on which your work is based.

\end{document}
