\documentclass[a4paper,11pt]{article}

\usepackage[big]{layaureo} 				%better formatting of the A4 page

\usepackage{graphics}
\usepackage{graphicx}

%Setup hyperref package, and colours for links
\usepackage{hyperref}

\begin{document}

%--------------------TITLE-------------

\title{Document Similarity using topic models and anchor words}
\author{Primal Pappachan, Sunil Gandhi, Ravendar Lal \\ 
\texttt{primal1@umbc.edu, sunilga1@umbc.edu, rlal1@umbc.edu}}
\date{\today}
\maketitle

%--------------------SECTIONS----------------------------------


\section{Progress Report}
Following is our progress report on the project
\subsection{Progress so far}

\begin{enumerate}
\item As per suggested in proposal, we talked to Dr. Oates on various aspects of this paper and he found our idea of approaching the problem quite convincing. He also suggested datasets for this task. Following are datasets that we  can use for our problem of topic modelling:

\begin{itemize}
\item New York Times Dataset
\item AP Press Dataset
\item NIPS Dataset
\end{itemize}

\item The authors of paper have mentioned that they have not practically implemented the algorithm and except that versions of algorithms are practical. We are trying to explore this possibility, and are looking at how we can take probabilistic approach and concepts introduced by the paper like NMF and anchor words to find topic models.

\item We explored other methods of topic modelling like LDA, PAM, Gibbs sampling and CTM tried to understand the need of new methodology for topic modelling. We enumerated the assumptions they made and complexity of each model.
\item We are exploring several libraries for LDA model and trying it on datasets mentioned above. This will be helpful for comparison with results of NMF algorithm.

\end{enumerate}

\subsection{Remaining Work}
\begin{enumerate}
\item Coding Non-negative Matrix Factorization (NMF): We still have to implement NMF algorithm.
\item We are trying to explore appropriate metric for comparison of topic models and how we can implement these metric.
\item We have to check the validity of claim that anchor words assumption and that these words form convex hull around all the words in vocabulary.
\item Finding distance between documents using probability distributions of document over topics.
\end{enumerate}


\subsection{Problems being faced}
\begin{enumerate}
\item This algorithm is based on assumption that document by word matrix is generated by product of word by topic and topic by document matrix. It turns out that this document by word is a very noisy version of product. 
\item There are huge number of words and words "the" doesn't affect the topics generated. But they increase size of matrix and thus the running time of algorithm considerably. The proposed solution for this problem is to remove these words and use TFIDF to run topic models for only words with certain TFIDF score.
\item We are still trying to figure out the best way to evaluate performance of different topic models. 
\end{enumerate}


\subsection{Schedule}
Following our schedule to tackle remaining tasks. 
\begin{table}
\centering
\begin{tabular}[htb]{l|r}
Task & Date\\\hline
Parsing Dataset & 3/30 \\
Implementing LDA & 4/2 \\
Implementing NMF & 4/8 \\
Result Comparison and Experimentation & 4/15 \\
Final Report and Presentation & 4/17
\end{tabular}
\caption{\label{tab:widgets}Schedule.}
\end{table}

\pagebreak

\section{Introduction}
A huge number of documents are generated daily in form of journals, papers from conferences and  news paper articles. A large amount of unstructured data is already present on web in form of wikipedia, web pages and social network sites. A user usually cannot read all the data available to him. Also, reading through data to find out more about document is not a very feasible option considering the number of documents available. I these cases it is important to understand the document and suggest users for relevant documents on certain topics or it would benefit if suggest the documents related to one that he is actually using. We think that topic models can be helpful in performing such tasks. \\

The other applications where we can use topic models is during searching. For example, topic models are ideal for searching on social media sites like quora. On quora there are large number of question and answer and it would be good if user could search through topics and find question and answers which are related to his topic of interest. Also, Finding advertisements which are related to query that user just searched and is probably interested in is an interesting problem whose roots lie in topic models. So, topic models have their application in various fields like advertisement, searching, recommendation engine, etc. \\

[1] suggests that problem of topic modelling can be formulated as problem of factorization of matrix into non-negative matrices.It also suggests use of anchor words to make this problem tractable. But, no empirical results of NMF and LDA which is another method of solving problem of topic models is given. Although they theoretically, suggest that NMF is better than LDA and it makes less severe assumption of separability. We will experimentally verify this claim and check for severity of "separability" assumption on real life dataset. We would also compare two documents based on there distribution over topic models. For this comparison, we can try different distance measures and understand which distance measure works best with topic models. This problem is significant because, understanding distance measures can be helpful in all types of machine learning problems like clustering, classification,etc. It will also be useful in understanding changes in document stream over time. Understanding these changes can be important for machine learning algorithms which work on this data as stated in [2]

\section{Previous Work}

The current approaches for learning Topic models use a Bayesian model based on the probability distribution of words inside a document{\cite{blei}}. Specifically the models used for document representation are

\begin{itemize} 
\item Pure model - based on the assumption that a document belongs to a single topic
\item Latent Dirichlet Allocation(LDA)
\item Corelated Topics model based on Pachinko Allocation
\end{itemize}
These models use algorithms which requires information about the maximum likelihood of observed data, can be hard to compute. Also methods like computing the Singular Value Decomposition(SVD) of the document corpus for learning topic models can result in unnecessary computations, they might give negative values in factored matrix as well. Instead, in this paper authors propose usage of NMF for learning topic models. They also suggest that if we assume existence of anchor words we can compute parameters of distribution of document in polynomial time. 

\section{Methods}
We will be implementing LDA and NMF models usiing anchor words assumption. We will be comparing results of both algorithms and try to evaluate which one words the best. In this way, we will be in position to comment on the work mentioned in this paper. 

So our main method of evaluation would be comparison of two algorithms. 


\begin{thebibliography}{99}
\bibitem{tm} \textit{Learning Topic Models---Going beyond SVD}. Arora, Saneev, Rong Ge, and Ankur Moitra. In FOCS 2012.
\bibitem{dred} \textit{Online methods for multi-domain learning and adaptation}. Dredze, Mark and Crammer, Koby. In
EMNLP, 2008.
\bibitem{nmf} \textit{Computing a nonnegative matrix factorization--provably}. Sanjeev Arora, Rong Ge, Ravi Kannan, Ankur Moitra. Proceedings of the 44th symposium on Theory of Computing. ACM, 2012.  
\bibitem{blei} \textit{Review article on Probabilistic topic models}. David m. Blei  
\end{thebibliography}




\pagebreak


\end{document}